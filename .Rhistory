# Data wrangling exercise
# This is my script file for chapter 5 data wrangling, done in week 4.
# Part 1. - create new script file is done, hence you're reading this :)
# Part 2. Read the two csv files
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
# Part 3. See the structure and dimensions of the data, and create summaries of the variables
str(hd)
head(hd)
summary(hd)
str(gii)
head(gii)
summary(gii)
# Part 4. Look at the meta files and rename the variables with (shorter) descriptive names
names(hd)
names(gii)
colnames(hd) <- c("HDI.Rank","Country","HDI","Life.Exp","Edu.Exp","Edu.Mean","GNI","GNI.Minus.Rank")
colnames(gii) <- c("GII.Rank", "Country", "GII","Mat.Mor","Ado.Birth","Parli.F","Edu2.F","Edu2.M","Labo.F","Labo.M")
# Part 5. Mutate the “Gender inequality” data and create two new variables. The first one should be the ratio of
# Female and Male populations with secondary education in each country. (i.e. edu2F / edu2M). The second new
# variable should be the ratio of labour force participation of females and males in each country
# (i.e. labF / labM).
gii <- mutate(gii, Edu2.FM = Edu2.F/Edu2.M)
gii <- mutate(gii, Labo.FM = Labo.F/Labo.M)
# Part 6. Join together the two datasets using the variable Country as the identifier. Keep only the countries
# in both data sets (Hint: inner join). The joined data should have 195 observations and 19 variables. Call the
# new joined data "human" and save it in your data folder.
library(dplyr)
human <- inner_join(hd, gii, by = "Country")
str(human) # making sure that the data has 195 obs and 19 variables
write.csv(human, file = "create_human.csv", row.names=FALSE)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
library(ggplot2); library(GGally); library(corrplot); library(tidyr); library(dplyr)
# access the MASS package
library(MASS)
# load the data
data("Boston")
# explore the dataset
str(Boston)
summary(Boston)
pairs(Boston)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)
# print the correlation matrix
cor_matrix
# visualize the correlation matrix
corrplot(cor_matrix, method="square", type="lower", cl.pos="b", tl.pos="d", tl.cex = 0.6)
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# number of rows in the Boston dataset
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1.5)
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
library(dplyr)
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
tt <- table(correct = correct_classes, predicted = lda.pred$class)
# dplyr, corrplot and GGally are available
library(dplyr)
library(corrplot)
library(GGally)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human <- select(human, -1)
summary(human)
# visualize the 'human' variables
ggpairs(human)
# dplyr, corrplot and GGally are available
library(dplyr)
library(corrplot)
library(GGally)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human <- select(human, -Country)
summary(human)
# visualize the 'human' variables
ggpairs(human)
# dplyr, corrplot and GGally are available
library(dplyr)
library(corrplot)
library(GGally)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human <- select(human, -Country)
summary(human)
# visualize the 'human' variables
ggpairs(human)
# Annukka Puotiniemi
# 20.11.2018
# Data wrangling exercise
# This is my script file for chapter 5 data wrangling, done in week 4.
# Part 1. - create new script file is done, hence you're reading this :)
# Part 2. Read the two csv files
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
# Part 3. See the structure and dimensions of the data, and create summaries of the variables
str(hd)
head(hd)
summary(hd)
str(gii)
head(gii)
summary(gii)
# Part 4. Look at the meta files and rename the variables with (shorter) descriptive names
names(hd)
names(gii)
colnames(hd) <- c("HDI.Rank","Country","HDI","Life.Exp","Edu.Exp","Edu.Mean","GNI","GNI.Minus.Rank")
colnames(gii) <- c("GII.Rank", "Country", "GII","Mat.Mor","Ado.Birth","Parli.F","Edu2.F","Edu2.M","Labo.F","Labo.M")
# Part 5. Mutate the “Gender inequality” data and create two new variables. The first one should be the ratio of
# Female and Male populations with secondary education in each country. (i.e. edu2F / edu2M). The second new
# variable should be the ratio of labour force participation of females and males in each country
# (i.e. labF / labM).
gii <- mutate(gii, Edu2.FM = Edu2.F/Edu2.M)
gii <- mutate(gii, Labo.FM = Labo.F/Labo.M)
# Part 6. Join together the two datasets using the variable Country as the identifier. Keep only the countries
# in both data sets (Hint: inner join). The joined data should have 195 observations and 19 variables. Call the
# new joined data "human" and save it in your data folder.
library(dplyr)
human <- inner_join(hd, gii, by = "Country")
str(human) # making sure that the data has 195 obs and 19 variables
write.csv(human, file = "create_human.csv", row.names=FALSE)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human
## ~ ~ End of Data Wrangling ~ ~
## ~ ~ Data Wrangling for Chapter 5 ~ ~
# Annukka Puotiniemi
# 21.11.2018
# Data wrangling exercise
# This is my script file for chapter 5 data wrangling, continued from last week.
# Link to the original data source (2 files, that I have already combined to the "human" data found below):
# http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv
# http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv
# Load the human data into R. Explore the structure and the dimensions of the data. Describe the data briefly.
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human1.txt", sep  =",", header = T)
str(human) # the data has 195 observations and 19 variables, as it should.
head(human)
summary(human)
names(human)
# This data originates from the United Nations Development Programme. The Human Development Index (HDI) was created
# to emphasize that, for an assesment of the development of a country, economic growth is not enough as a measure.
# Instead, people and their capabilities should be the assesment criteria. The HDI is made up of three indices:
# Life Expectancy Index, Education Index, and GNI Index.
# More information about the index can be found here: http://hdr.undp.org/en/content/human-development-index-hdi
# The data combines several indicators, collected from most countries in the world. The descriptions of the
# variables can be found here: https://raw.githubusercontent.com/TuomoNieminen/Helsinki-Open-Data-Science/master/datasets/human_meta.txt
# 1. Mutate the data: transform the Gross National Income (GNI) variable to numeric
library(stringr)
str_replace(human$GNI, pattern=",", replace ="") %>% as.numeric$GNI
human$GNI <- as.numeric(human$GNI)
human$GNI
# 2. Exclude unneeded variables: keep only the columns matching the following variable names (described in the meta
# file above):  "Country", "Edu2.FM", "Labo.FM", "Edu.Exp", "Life.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F"
# columns to keep
keep <- c("Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
# select the 'keep' columns
human <- select(human, one_of(keep))
# 3. Remove all rows with missing values
# print out a completeness indicator of the 'human' data
complete.cases(human)
# print out the data along with a completeness indicator as the last column
data.frame(human[-1], comp = complete.cases(human))
# filter out all rows with NA values
human <- filter(human, complete.cases(human))
# 4. Remove the observations which relate to regions instead of countries
# look at the last 10 observations
tail(human, 10)
# last indice we want to keep
last <- nrow(human) - 7
# choose everything until the last 7 observations
human <- human[1:last, ]
# 5. Define the row names of the data by the country names and remove the country name column from the data.
# The data should now have 155 observations and 8 variables.
# add countries as rownames
rownames(human) <- human$Country
# remove the Country variable
human <- select(human, -Country)
str(human) # 155 observations and 8 variables, correct!
# Save the human data in your data folder including the row names. You can overwrite your old ‘human’ data.
write.csv(human, file = "create_human.csv", row.names=TRUE)
## ~ ~ End of Data Wrangling ~ ~
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# dplyr, corrplot and GGally are available
library(dplyr)
library(corrplot)
library(GGally)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human <- select(human, -1)
summary(human)
# visualize the 'human' variables
ggpairs(human)
# compute the correlation matrix and visualize it with corrplot
cor_human <- cor(human)
cor_human
corrplot(cor_human, method="square", type="lower", cl.pos="b", tl.pos="d", tl.cex = 0.6)
# cor(human) %>% corrplot was the code given in DataCamp, but I find it slightly confusing to present the data in that way, I prefer the plot I wrote above.
pca_human <- prcomp(human)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.4, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2], caption = "Table with stuff"))
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2], caption = "Table with stuff")
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
pca_human
human
humans <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human1.txt", sep  =",", header = T)
library(stringr)
str_replace(humans$GNI, pattern=",", replace ="") %>% as.numeric$GNI
humans$GNI <- as.numeric(human$GNI)
keep <- c("HDI", Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
humans <- select(humans, one_of(keep))
complete.cases(humans)
data.frame(humans[-1], comp = complete.cases(humans))
humans <- filter(humans, complete.cases(humans))
tail(humans, 10)
last <- nrow(humans) - 7
humans <- humans[1:last, ]
rownames(humans) <- humans$Country
humans <- select(humans, -Country)
str(humans)
write.csv(humans, file = "create_humans.csv", row.names=TRUE)
humans <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human1.txt", sep  =",", header = T)
library(stringr)
str_replace(humans$GNI, pattern=",", replace ="") %>% as.numeric$GNI
humans$GNI <- as.numeric(human$GNI)
keep <- c("HDI", "Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
humans <- select(humans, one_of(keep))
complete.cases(humans)
data.frame(humans[-1], comp = complete.cases(humans))
humans <- filter(humans, complete.cases(humans))
tail(humans, 10)
last <- nrow(humans) - 7
humans <- humans[1:last, ]
rownames(humans) <- humans$Country
humans <- select(humans, -Country)
str(humans)
write.csv(humans, file = "create_humans.csv", row.names=TRUE)
humans <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human1.txt", sep  =",", header = T)
library(stringr)
str_replace(humans$GNI, pattern=",", replace ="") %>% as.numeric$GNI
humans$GNI <- as.numeric(human$GNI)
keep <- c("HDI", "Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
humans <- select(humans, one_of(keep))
complete.cases(humans)
data.frame(humans[-1], comp = complete.cases(humans))
humans <- filter(humans, complete.cases(humans))
tail(humans, 10)
last <- nrow(humans) - 7
humans <- humans[1:last, ]
rownames(humans) <- humans$Country
humans <- select(humans, -Country)
str(humans)
write.csv(humans, file = "create_humans.csv", row.names=TRUE)
humans$devt[humans$HDI>=0.800]<-"Very High"
humans$devt[humans$HDI>=0.700 & humans$HDI<0.799]<-"High"
humans$devt[humans$HDI>=0.550 & humans$HDI<0.699]<-"Medium"
humans$devt[humans$HDI<0.550]<-"Low"
humans
pca_humans <- prcomp(humans, scale. = TRUE)
# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = humans$HDI, xlab = "PC1", ylab = "PC2")
pca_humans <- prcomp(humans -HDI, scale. = TRUE)
humans
pca_humans <- prcomp(humans -devt, scale. = TRUE)
pca_humans <- prcomp(humans -$devt, scale. = TRUE)
pca_humans <- prcomp(~ . -devt -HDI, data=humans, scale. = TRUE)
humans
pca_humans <- prcomp(~ . -devt -HDI -Country, data=humans, scale. = TRUE)
pca_humans <- prcomp(~ . -devt -HDI -Country -GNI, data=humans, scale. = TRUE)
pca_humans
# create and print out a summary of pca_human
s <- summary(pca_humans)
s
# draw a biplot
biplot(pca_humans, cex = c(0.6, 1), col = humans$HDI, xlab = "PC1", ylab = "PC2")
pca_humans <- prcomp(~ . -devt -HDI -Country -GNI, data=humans, scale. = TRUE)
pca_humans
# create and print out a summary of pca_human
s <- summary(pca_humans)
s
# draw a biplot
biplot(pca_humans, cex = c(0.6, 1), xlab = "PC1", ylab = "PC2")
pca_humans <- prcomp(~ . -devt -HDI -Country -GNI, data=humans, scale. = TRUE)
pca_humans <- pca_humans +devt
pca_humans <- prcomp(~ . -HDI -Country -GNI, data=humans, scale. = TRUE)
humans$devt[humans$HDI>=0.800]<-1
humans$devt[humans$HDI>=0.700 & humans$HDI<0.799]<-2
humans$devt[humans$HDI>=0.550 & humans$HDI<0.699]<-3
humans$devt[humans$HDI<0.550]<-4
pca_humans <- prcomp(~ . -HDI -Country -GNI, data=humans, scale. = TRUE)
humans
pca_humans <- prcomp(~ . -HDI -Country -GNI, data=humans, scale. = TRUE)
pca_humans <- prcomp(~ . -HDI -Country -GNI, data=humans, scale. = TRUE)
s <- summary(pca_humans)
s
biplot(pca_humans, cex = c(0.6, 1), xlab = "PC1", ylab = "PC2")
biplot(pca_humans, cex = c(0.6, 1), col = humans$devt, xlab = "PC1", ylab = "PC2")
biplot(pca_humans, cex = c(0.6, 1), colour = 'devt', xlab = "PC1", ylab = "PC2")
warnings()
biplot(pca_humans, cex = c(0.6, 1), col = 'devt', xlab = "PC1", ylab = "PC2")
biplot(pca_humans, cex = c(0.6, 1), col = devt, xlab = "PC1", ylab = "PC2")
biplot(pca_humans, cex = c(0.6, 1), col = humans$devt, xlab = "PC1", ylab = "PC2")
autoplot(prcomp(~ . -HDI -Country -GNI), data = humans, colour = 'devt')
install.packages("devtools")
library(devtools)
install_github("ggbiplot", "vqv")
library(ggbiplot)
g <- ggbiplot(pca_humans, obs.scale = 1, var.scale = 1,
groups = humans$devt, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
g <- ggbiplot(pca_humans, obs.scale = 1, var.scale = 1,
groups = humans$devt, ellipse = TRUE,
circle = TRUE)
library(ggbiplot)
install_github("ggbiplot", "vqv")
install.packages("devtools")
library(devtools)
install_github("ggbiplot", "vqv")
install_github("ggbiplot", "vqv")
install.packages("R6")
install.packages("R6")
library(devtools)
install_github("ggbiplot", "vqv")
library(devtools)
install_github("ggbiplot", "vqv", lib="C:/Users/annuk/OneDrive/Uni of Helsinki/Introduction to Opean Data Science/IODS-project/data")
install_github("ggbiplot/vqv")
githubinstall("ggbiplot")
install_github("vqv/ggbiplot")
library(ggbiplot)
remove.packages("Rcpp")
