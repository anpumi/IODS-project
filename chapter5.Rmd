
# 5. Dimensionality reduction techniques

##  Part 1. Overview of the Data

For clarity, here are the variables and their short explanations:   
* `Life.Exp` = Life expectancy at birth  
* `Edu.Exp` = Expected years of schooling    
* `GNI` = Gross National Income  
* `Mat.Mor` = Maternal mortality ratio  
* `Ado.Birth` = Adolescent birth rate  
* `Parli.F` = Percentage of female representatives in parliament  
* `Edu2.FM` = Proportion of females with at least secondary education / Proportion of males with at least secondary education  
* `Labo.FM` = Proportion of females in the labour force / Proportion of males in the labour force  

```{r}
# dplyr, corrplot and GGally are available
library(dplyr)
library(corrplot)
library(GGally)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human <- select(human, -1)
summary(human)
```


Studying the output of the `summary`function we can see the distributions of the variables. For example, the average life expectancy is 71.65 years with a minimum of 49 and maximum of 83.5 years.  

Next, we will study a graphical overview of the data. I will do this by drawing a `ggpairs` plot. We will also draw a correlation plot.  

```{r}

# visualize the 'human' variables
ggpairs(human)

# compute the correlation matrix and visualize it with corrplot
cor_human <- cor(human)
cor_human
corrplot(cor_human, method="square", type="lower", cl.pos="b", tl.pos="d", tl.cex = 0.6)
# cor(human) %>% corrplot was the code given in DataCamp, but I find it slightly confusing to present the data in that way, I prefer the plot I wrote above.

```

Let's look at the information for the distribution of each variable.  All the variables are unimodally distributed (they have one peak).  
* `Edu2.FM` left-skewed distribution  
* `Labo.FM` left-skewed distribution  
* `Life.Exp` left-skewed distribution  
* `Edu.Exp` slightly left-skewed distribution, almost symmetric  
* `Mat.Mor` right-skewed distribution  
* `Ado.Birth` right-skewed distribution  
* `Parli.F` right-skewed distribution  

The plot above shows that the highest correlation (in absolute values) is between the variables `Mat.Mor` and `Life.Exp`, with a negative correlation of 0.857. Other very high correlation pairs include:  
* `Edu.Exp` and `Life.Exp` (positive)  
* `Mat.mor` and `Edu.Exp` (negative)  
* `Ado.Birth` and `Life.Exp` (negative)  
* `Ado.Birth` and `Edu.Exp` (negative)  
* `Ado.Birth` and `Mat.Mor` (positive)  

The `corrplot`above shows a visual representation of these correlations very clearly too. As we learned last week, positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the square are proportional to the correlation coefficients.  

Particularly low correlations occur with the "percentage of female representatives in parliament" variable.  

As we can see again, the variables have some rather high correlations between them. I feel that now, however, is a good time to remind about the fact that correlation does *not* equal causality, and that in social sciences particularly, the relationships between various variables are much more complex than often meets the eye.  

***

## Part 2. Principal Component Analysis (PCA)

In PCA, we decompose a data matrix into smaller matrices, allowing us to extract the underlying *principal* components. Ideally, the variance along these principal components is a reasonable characterization of the complete data set.  

There are two different methods of PCA (from linear algebra), the Eigenvalue Decomposition and the Singular Value Decomposition (SVD). The `prcomp()`function in R uses the SVD, which is the more accurate, and therefore preferred method of PCA.  

We will now perform SVD PCA on the `human`dataset. We will first do this for the non-standardized data.  

```{r}
pca_human <- prcomp(human)
pca_human
```

This dataset has eight principal components. The analysis first shows the standard deviations of the components, and then the variability.  
The first principal component `PC1` captures the maximum amount of variance form the features in the original data.  
`PC2` is statistically independent from the first, and captures the maximum variability that is left.  
The same applies for rest of the principal components: they are all non-correlated and each is less important than the last one in terms of variance captured.  
Now, let's look at a biplot of the above data.  

```{r}
# create and print out a summary of pca_human
s <- summary(pca_human)
s

# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1) 

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])

```

As a note about this biplot, the observations by the first two principal components are displayed on the x- and y-axis, and the arrows are used to visualize the connections between the original variables and the PC's.

The angles between the arrows that represent the original variables show the correlations between the variables. Small angle represents a high positive correlation.

The angle between a variable and the PC axis show the correlation between the two. Again, a small angle represents a high positive correlation.

The length of the arrows proportionally show us the standard deviation of the variables.

***

## Part 3. Principal Component Analysis (PCA) with Standardized Variables

It is worth noting that PCA is sensitive to scaling and assumes that features (variables) with larger variances are more important than those with smaller variances. Therefore, scaling the variables used in PCA is a good idea. Let's do that now.

```{r}
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human

# create and print out a summary of pca_human
s <- summary(pca_human)
s

# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1) 

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])

```


The first biplot is very difficult to read and therefore interpret, and unfortunately at present I lack the skills to make it more readable. However, the results of both analysis are clearly different. The standardized analysis is the one we should concentrate on, and luckily that is the one where the biplot is readable. The reason for concentrating on the standardized analysis (and the reason why the analysis are different) was mentioned above: PCA is sensitive to scaling and assumes that features (variables) with larger variances are more important than those with smaller variances.

Including the percentages for the first two principal components in the biplot tells us that the first two principal components `PC1` and `PC2` account for 98.3% of variance in the original data in the non-standardized analysis and for 64.5% in the standardized version. This again demonstrates why standardizing the variables is a good idea: in the non-standardized version the model places too much importance on the larger variances, making the entire model badly skewed. We would ideally include more PC's in our standardized analysis to capture more of the variance. The PC's selected for the analysis should capture around 80% of the variance to be reliable, therefore 3 PC's (77%) or 4 PC's (86%) would be good. 

Next, we will look at the correlations. If we look at, for example, the angle of the arrows that represent `Mat.Mor` and `Ado.Birth`, we can see that the angle is quite small between the two arrows. This corresponds to a high positive correlation (0.7586615). The angles of the arrows are the same in both biplots, since the correlations are identical for standardized and non-standardized data.

The lengths of the arrows - representing the standard deviations of the variables - vary significantly between the non-standardized and standardized analyses. This is due to the fact that the SD results change dramatically once we standardize our data. The standard deviations are much more uniform in size in the standardized analysis.

Standardize the variables in the human data and repeat the above analysis. Interpret the results of both analysis (with and without standardizing). Are the results different? Why or why not? Include captions (brief descriptions) in your plots where you describe the results by using not just your variable names, but the actual phenomenons they relate to.

***

## Part 4. Interpretations of `PC1` and `PC2` Dimensions

Looking at the biplot drawn after PCA on the standardized human data, we can see that there is one major cluster (call this `C1`), and a smaller cluster almost directly below it (`C2`). There is also a more scattered cluster diagonally to the top-right from the main cluster (`C3`). The rest of the countries (shown as numbers) are scattered around more or less randomly. 

Since the clusters of countries `C1` and `C3` are different based on `PC1`, such differences are likely to be due to the variables that have heavy influences on `PC1`. Those variables, as can be seen in the PCA table, are `Life.Exp`, `Edu.Exp`, `Mat.Mor` and `Ado.Birth`.

As the `C1` and `C2` clusters are different based on `PC2`, then the variables that heavily influence `PC2` are likely to be responsible. Those variables, per the PCA table, are `Labo.FM` and `Parli.F`.

The arrows in relation to one another can be interpreted as follows:  

> When two arrows form a small angle, the variables are positively correlated.  
Example: `Mat.Mor` and `Ado.Birth` (0.76 correlation).  
> When they meet each other at around 90°, they are not likely to be correlated.  
Example: `Edu2.FM` and `Labo.FM` (0.01).  
> When they form a large angle (close to 180°), they are negatively correlated.  
Example: `Mat.Mor` and `Life.Exp` (-0.86).  

So, let's hazard a guess at what some of this means. 

Looking at the countries in cluster 3, we find countries in Sub-Saharan Africa: Senegal, Malawi, Lesotho, Ethiopia and Kenya. Some of the countries in cluster 1 include Cyprus, Lithuania, and Thailand. Seeing these countries now helps the whole scenario make more sense: very poor countries are in one cluster, separated from better-to-do countries by variances in aspects such as maternal mortality and life expectancy. The point is further illuminated as we look for the richer countries in the world, such as Norway and Denmark, and find them even further away to the left on the `PC1` axis from the Sub-Saharan countries. Hence we can probably conclude that the differences in these variables separate rich, middle and poor countries in the world.

But what about the `PC2` axis? There, in cluster 2, are countries such as Cyprus and Sri Lanka, who are separated by the variances in "percentage of female representatives in parliament" and "proportion of females to males in the workforce" variables, from the fairly-well-to-do countries mentioned above.

The observations I've made above got me thinking about changing the plot further. The countries in the HDI index are listed according to the Human Development Index score they received. How about if we color-code the countries according to the scores they received. What would the plot reveal then?

In the [HDI technical notes](http://hdr.undp.org/sites/default/files/hdr2018_technical_notes.pdf), the following groupings can be found:

> Very high human development:   0.800 and above  
> High human development:        0.700-0.799  
> Medium human development:      0.550-0.699  
> Low human development:         Below 0.550  

So, let's use those groupings to color our countries.

First things first (and bear with me here, this might take up some space), I need to create a new dataset `humans`that will include the HDI score. I did this in a separate R script file to save space, hence the hashtags below...

```{r}
# humans <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human1.txt", sep  =",", header = T)
library(stringr)
library(dplyr)
# str_replace(humans$GNI, pattern=",", replace ="") %>% as.numeric$GNI
# humans$GNI <- as.numeric(humans$GNI)
# keep <- c("HDI", "Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
# humans <- select(humans, one_of(keep))
# complete.cases(humans)
# data.frame(humans[-1], comp = complete.cases(humans))
# humans <- filter(humans, complete.cases(humans))
# tail(humans, 10)
# last <- nrow(humans) - 7
# humans <- humans[1:last, ]
# rownames(humans) <- humans$Country
# humans <- select(humans, -Country)
# str(humans)
# write.csv(humans, file = "create_humans.csv", row.names=TRUE)
```

Then, we need to add a column to our `humans` dataset that specifies the HDI quality as per the technical notes.

```{r}
humans <- read.table("create_humans.csv", sep = "," , header=TRUE)
humans$devt[humans$HDI>=0.800]<-"Very High"
humans$devt[humans$HDI>=0.700 & humans$HDI<0.799]<-"High"
humans$devt[humans$HDI>=0.550 & humans$HDI<0.699]<-"Medium"
humans$devt[humans$HDI<0.550]<-"Low"

```

Now we are ready to standardize the dataset, perform PCA (excluding `HDI` as it wasn't part of the previous analysis set, and `devt` as it is only required for classification purposes), and draw the biplot.

```{r}
pca_humans <- prcomp(~ . -X -HDI -devt, data = humans, scale. = TRUE)

# create and print out a summary of pca_human
s <- summary(pca_humans)
s
```

Then, as I was trying to get the plot coloured as I wanted it, I came across this excellent [tutorial](https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/)!

```{r}
# First I needed to access some stuff
# install.packages("devtools")
# install.packages("R6")
library(devtools)
# devtools::install_github("vqv/ggbiplot")

library(ggbiplot)
```


```{r}
g <- ggbiplot(pca_humans, obs.scale = 1, var.scale = 1, 
              groups = humans$devt, ellipse = TRUE, 
              circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'vertical', 
               legend.position = 'right')
print(g)
```

Now, this plot I find much easier to interpret. The colors shows us the clusters much more easily, and I feel they are needed as the previous method did not make seeing the clusters very easy at all. This is largely due to the fact that the clusters are not tightly clustered.

Most differences are the result the variables that have heavy influences on `PC1` axis. As observed previously, those variables are `Life.Exp`, `Edu.Exp`, `Mat.Mor` and `Ado.Birth`. It makes perfect sense that countries with a higher score in the Human Development Index are furthest away from the lowest scoring countries, and they are ordered (as expected) `Very High`-> `High` -> `Medium`-> `Low`on the first principal component axis. It is worth noting that the low HDI index countries are much more spread out on the plot, indicating more variances between their variables than in those of the higher HDI score countries. In fact, the higher in the index we go, the tighter the clusters appear.

A couple of noteworthy points about the above plot:  
- The data ellipses capture 68% (default) for each of the 4 HDI indicators in the data.
- The large circle shows the theoretical maximum reach of the arrows. 

***

## Part 5. It's MCA Tea Time!

> Let's have some tea, shall we!

Multiple Correspondence Analysis (MCA) is an extension of Correspondence Analysis (CA). It allows us to analyze the pattern of relationships found within several categorical dependent variables.

MCA can also be viewed as a generalization of PCA: variables analyzed are **categorical** instead of **quantitative**. A very helpful article on MCA in R can be found [here](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/).

So, let's put MCA to use to find groups of individuals with similar profiles in their answers to the survey questions and also to find the associations between variable categories.

First, we will access some packages and the `tea`dataset. The original dataset consists of 300 observations and 36 variables. The data is the results of a survey of tea drinkers.  
The `?tea` query gives us the following information:  

> "The data used here concern a questionnaire on tea. We asked to 300 individuals how they drink tea (18 questions), what are their product's perception (12 questions) and some personal details (4 questions)."

AND

>  "A data frame with 300 rows and 36 columns. Rows represent the individuals, columns represent the different questions. The first 18 questions are active ones, the 19th is a supplementary quantitative variable (the age) and the last variables are supplementary categorical variables."
We will choose which columns to keep for our dataset, look at the summaries and structure of the data, and then do some simple visualizations.

```{r}
# the tea dataset and packages FactoMineR, ggplot2, dplyr and tidyr are available
library(FactoMineR)
data(tea)
library(ggplot2)
library(dplyr)
library(tidyr)

# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)

# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

```

The data frame now consists of 300 observations and  6 variables. The bar plots nicely help visualize the spread of each variable.

Now we will perform the MCA and print its summary, as well as visualize it.

```{r}
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
```

Let's look at the output of the MCA summary first.  

The `Eigenvalues` section at the top show the variances and the percentages of variances retained by each dimension.  

Then in the next section - `Individuals` - we have the individuals coordinates (e.g. `Dim.1`), the individuals percentage contribution on each dimension (`ctr`) and the the squared correlations (`cos2`) on the dimensions.  

The `Categories` table specifies the coordinates of the variable categories (e.g. `Dim.1`), the contribution percentage (`ctr`), the squared correlations (`cos2`) and the `v.test` value. The v.test follows normal distribution: if the value is ± 1.96, the coordinate is significantly different from zero. We can see that this is the case for all of the listed categories except `alone` and `other`.  

Finally, we have the `Categorical variables (eta2)`. It shows the squared correlation between each variable and the dimension. A value close to one indicates a strong connection between the variable and the dimension. Variables `how`and `where`, although not *very* close to one, are the closest at around 0.7, on the first dimension.

We can use the `invisible` argument in the code to choose whether to plot individuals, variances, or both. We can do this by changing the argument to "none", "ind", or "var". (There is also the option of plotting the supplementary or background variables). By turning "ind" invisible, the above plots variances.

The MCA factor map can be interpreted by looking at the distance between variable categories. This shows a measure of their similarity. In our plot, the variables `chain store`and `tea bag`have a high degree of similarity. This tells us that individuals who buy their tea from chain stores drink mainly teabag tea, and vice versa. `black`and `no sugar`are also very similar, indicating that people who do not use sugar in their tea also do not use milk.

Let's plot individuals and then both.  

Note: The `habillage`argument can be changed to specify the qualitative variable (by its index or name), and used for coloring individuals by groups.

I experimented with all the variables as the `habillage` argument, and found that the clearest groupings were found by using `how`and `where` (note that these are the same variables that were closest to one in categorical variables). In fact, those two yielded remarkably similar results. For this analysis, I chose `how`. Let's see how `how`makes our plot look:

```{r}
plot(mca, invisible=c("var"), habillage = "how")
```

Let's also look at the plot that includes both the variables and the individuals.

```{r}
plot(mca, invisible=c("none"), habillage = "how")
```

What if we want to run MCA on the entire `tea`dataset? By accessing `?tea` we find the following code for MCA. The arguments used specify that variable number 19 is a supplementary quantitative variable and that arguments 20 through to 36 are supplementary variables. Therefore it follows that variables 1 through to 18 are active variables.

The following code now excludes `graph = FALSE` and therefore the default graphs are displayed.

```{r}
res.mca=MCA(tea,quanti.sup=19,quali.sup=20:36)
```

Let's next look at how these plots can be interpreted. A very helpful video can be found [here](https://www.youtube.com/watch?v=reG8Y9ZgcaQ).

`Graph 1: MCA factor map` - This plot shows the individuals. There isn't a clear cluster of individuals here, so there isn't much to say about this plot.

Now bear with me please while I present the graphs out of order, Graph 2 will come last, and we'll soon see why.

`Graph 3: Plot of the variables` - this graph shows which variables are connected to the two dimensions. In our example, the variable `where`is linked to both dimensions, as are `price`and `how` (just not as strongly). We can also see that at the bottom left of the graph there is a whole bunch of variables that are not at all strongly linked to the dimensions.

`Graph 4: Supplementary variables on the MCA factor map` - graph and the circle within it are similar to the one in PCA. If the arrow is close to the circle, then the correlation between the variable and the two dimensions is strong. As we can see in our example, `age`clearly is not strongly correlated with the dimensions in question.

`Graph 2: MCA factor map` - This graph of categories is more interesting than the second graph. However, the problem here is that there are many categories, making the graph difficult to read. We can use the `FactoMineR`function to make the information more legible. Let's do that below. I have included code comments below.

```{r}
plot(res.mca, invisible=c("ind", "quali.sup"), selectMod="contrib 10", cex=0.8) 

# here we first code the plot, making ind and quali.sup invisible, just like we did in the example given in DataCamp. This time we also make the font smaller for improved readability. Next, we choose the 10 categories that are the best represented on the 2 dimension. Please note that the points that are no longer included are still drawn transparent. 
```

The above chart now shows us the ten categories that are the best represented on the 2 dimension. I find it quite interesting that a `price`category factor `p_upscale` is this strongly present on the second dimension, along with the factor `tea shop` from the `where` category and the factor `unoackaged` from the `how` category.

Finally, let's look at one more plot, just out of curiosity (and hope we don't end the way of the proverbial cat :) ).

```{r}
plot(res.mca, invisible = c("var","quali.sup"), habillage = "frequency")
```

This plot is rather handy in seeing certain characteristics of survey respondents in graphical form. As we can see, the colour green seems to dominate, indicating that most of the `tea`survey respondents like to drink a nice cuppa more than twice a day. I suspect that this survey was carried out in Britain, and if this was the case, I do not doubt the accuracy of this assessment one bit!

That's all for this week. Thanks so much for reading this to the very end, and have a great week!