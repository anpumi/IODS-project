
# 5. Dimensionality reduction techniques

##  Part 1. Overview of the Data

For clarity, here are the variables and their short explanations:  
- `Life.Exp` = Life expectancy at birth
- `Edu.Exp` = Expected years of schooling 
- `GNI` = Gross National Income
- `Mat.Mor` = Maternal mortality ratio
- `Ado.Birth` = Adolescent birth rate 
- `Parli.F` = Percetange of female representatives in parliament 
- `Edu2.FM` = Proportion of females with at least secondary education / Proportion of males with at least secondary education
- `Labo.FM` = Proportion of females in the labour force / Proportion of males in the labour force

```{r}
# dplyr, corrplot and GGally are available
library(dplyr)
library(corrplot)
library(GGally)
human <- read.table("create_human.csv", sep = "," , header=TRUE)
human <- select(human, -1)
summary(human)
```


Studying the output of the `summary`function we can see the distributions of the variables. For example, the average life expectancy is 71.65 years with a minimum of 49 and maximum of 83.5 years.

Next, we will study a graphical overview of the data. I will do this by drawing a `ggpairs` plot. We will also draw a correlation plot.

```{r}

# visualize the 'human' variables
ggpairs(human)

# compute the correlation matrix and visualize it with corrplot
cor_human <- cor(human)
cor_human
corrplot(cor_human, method="square", type="lower", cl.pos="b", tl.pos="d", tl.cex = 0.6)
# cor(human) %>% corrplot was the code given in DataCamp, but I find it slightly confusing to present the data in that way, I prefer the plot I wrote above.

```

Let's look at the information for the distribution of each variable.  All the variables are unimodally distributed (they have one peak).
- `Edu2.FM` left-skewed distribution
- `Labo.FM` left-skewed distribution
- `Life.Exp` left-skewed distribution
- `Edu.Exp` slightly left-skewed distribution, almost symmetric
- `Mat.Mor` right-skewed distribution
- `Ado.Birth` right-skewed distribution
- `Parli.F` right-skewed distribution

The plot above shows that the highest correlation (in absolute values) is between the variables `Mat.Mor` and `Life.Exp`, with a negative correlation of 0.857. Other very high correlation pairs include:
- `Edu.Exp` and `Life.Exp` (positive)
- `Mat.mor` and `Edu.Exp` (negative)
- `Ado.Birth` and `Life.Exp` (negative)
- `Ado.Birth` and `Edu.Exp` (negative)
- `Ado.Birth` and `Mat.Mor` (positive)

The `corrplot`above shows a visual representation of these correlations very clearly too. As we learned last week, positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the square are proportional to the correlation coefficients. 

Particularly low correlations occur with the "percetange of female representatives in parliament" variable.

As we can see again, the variables have some rather high correlations between them. I feel that now, however, is a good time to remind about the fact that correlation does *not* equal causality, and that in social sciences particularly, the relationships betwen various variables are much more complex than often meets the eye.

## Part 2. Principal Component Analysis (PCA)

In PCA, we decompose a data matrix into smaller matrices, allowing us to extract the underlying *principal* components. Ideally, the variance along these principal components is a reasonable characterization of the complete data set.

There are two different methods of PCA (from linear algebra), the Eigenvalue Decomposition and the Singular Value Decomposition (SVD). The `prcomp()`function in R uses the SVD, which is the more accurate, and therefore preferred method of PCA.

We will now perform SVD PCA on the `human`dataset. We will first do this for the non-standardized data.

```{r}
pca_human <- prcomp(human)
pca_human
```

This dataset has eight principal components. The analysis first shows the standard deviations of the components, and then the variability.
The first principal component `PC1` captures the maximum amount of variance form the features in the original data.
`PC2` is statistically independent from the first, and captures the maximum variability that is left.
The same applies for rest of the principal components: they are all non-correlated and each is less important than the last one in terms of variance captured.
Now, let's look at a biplot of the above data.

```{r}
# create and print out a summary of pca_human
s <- summary(pca_human)
s

# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1) 

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])

```

As a note about this biplot, the observations by the first two principal components are displayed on the x- and y-axis, and the arrows are used to visualize the connections between the original variables and the PC's.

The angles between the arrows that represent the original variables show the correlations between the variables. Small angle represents a high positive correlation.

The angle between a variable and the PC axis show the correlation between the two. Again, a small angle represents a high positive correlation.

The length of the arrows proportionally show us the standard deviation of the variables.

## Part 3. Principal Component Analysis (PCA) with Standardized Variables

It is worth noting that PCA is sensitive to scaling and assumes that features (variables) with larger variances are more important than those with smaller variances. Therefore, scaling the variables used in PCA is a good idea. Let's do that now.

```{r}
# human_std <- scale(human) was the way that I have scaled in the previous weeks. Now I found the following:
pca_human <- prcomp(human, scale. = TRUE)
pca_human

# create and print out a summary of pca_human
s <- summary(pca_human)
s

# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1) 

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])

```


The first biplot is very difficult to read and therefore interpret, and unfortunately at present I lack the skills to make it more readable. However, the results of both analysis are clearly different. The standardized analysis is the one we should concentrate on, and luckily that is the one where the biplot is readable. The reason for concentrating on the standardized analysis (and the reason why the analysis are different) was mentioned above: PCA is sensitive to scaling and assumes that features (variables) with larger variances are more important than those with smaller variances.

Including the percentages for the first two principal components in the biplot tells us that the first two principal components `PC1` and `PC2` account for 98.3% of variance in the original data in the non-standardized analysis and for 64.5% in the standardized version. Therefore we would include more PC's in our standardized analysis, maybe 4 principal components, to capture more of the variance. This again demonstrates why standardizing the variables is a good idea: in the non-standardized version the model places too much importance on the larger variances, making the entire model badly skewed.

Next, we will look at the correlations. If we look at, for example, the angle of the arrows that represent `Mat.Mor` and `Ado.Birth`, we can see that the angle is quite small between the two arrows. This corresponds to a high positive correlation (0.7586615). The angles of the arrows are the same in both biplots, since the correlations are identical for standardized and non-standardized data.

The lengths of the arrows - representing the standard deviations of the variables - vary significantly between the non-standardized and standardized analyses. This is due to the fact that the SD results change dramatically once we standardize our data. The standard deviations are much more uniform in size in the standardized analysis.

Standardize the variables in the human data and repeat the above analysis. Interpret the results of both analysis (with and without standardizing). Are the results different? Why or why not? Include captions (brief descriptions) in your plots where you describe the results by using not just your variable names, but the actual phenomenons they relate to.

## Part 4. Interpretations of `PC1` and `PC2` Dimensions

Give your personal interpretations of the first two principal component dimensions based on the biplot drawn after PCA on the standardized human data.

The angle between the arrows can be interpret as the correlation between the variables.
The angle between a variable and a PC axis can be interpret as the correlation between the two.
The length of the arrows are proportional to the standard deviations of the variables

## Part 5.

```{r}
# the tea dataset and packages FactoMineR, ggplot2, dplyr and tidyr are available
library(FactoMineR)
data(tea)
library(ggplot2)
library(dplyr)
library(tidyr)

# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)

# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

```


Load the tea dataset from the package Factominer. Explore the data briefly: look at the structure and the dimensions of the data and visualize it. 

```{r}
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
```


Then do Multiple Correspondence Analysis on the tea data (or to a certain columns of the data, it's up to you). Interpret the results of the MCA and draw at least the variable biplot of the analysis. You can also explore other plotting options for MCA. Comment on the output of the plots. 