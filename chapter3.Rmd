# 3. Logistic regression

*Below are all the codes, my interpretations and explanations for this week's data analysis exercises.*

The data for this exercise was downloaded from the UC Irvine Machine Learning Repository. 

The data are two student performance data sets that approach student achievement in secondary education of two schools in Portugal. The data variables include student grades, as well as demographic, social and school related features. The data was collated using school reports and questionnaires. The two datasets that analyze the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por).

In the data wrangling exercise (which can be found in my [GitHub repository](https://github.com/msnxt/IODS-project/blob/master/create_alc.R)) performed prior to this report, the two data sets were combined using a number of variables as student identifiers. Only the students present in both data sets were kept. Next, the average of the answers related to weekday and weekend alcohol consumption was taken to create a new variable 'alc_use'. Then 'alc_use' was used to create a new logical column 'high_use' (TRUE for students whose 'alc_use' is greater than 2 and FALSE otherwise). 

Citation:  
P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. 


## Part 1 - General Housekeeping

For the first part of this analysis I created a new RMarkdown file and save it as an empty file named 'chapter3.Rmd'. Then I included this file as a child file in my 'index.Rmd' file, and as a result you are now reading this.


## Part 2 - The Data Set

I read the joined student alcohol consumption data into R from my local folder.

```{r}
alc <- read.table("create_alc.csv", sep = "," , header=TRUE)
```

Next, I printed out the names of the variables in the data.

```{r}
colnames(alc)

```

The data set consists of 382 observations of 35 variables. The variables are a combination of numeric, nominal and binary attributes.


## Part 3 - Choosing Variables of Interest

For this part, in order to study the relationship between high or low alcohol consumption, I chose the following 4 variables from the data, including my personal hypothesis for each variable:  
1. `age` - I hypothesize that higher age will correlate with high alcohol consumption.  
2. `failures` - indicating the number of past class failures. My hypothesis is that a high number of failures correlates with high alcohol use.  
3. `famrel` - indicator for the quality of family relations. Better family relations will mean lower alcohol consumption and conversely worse relations will correlate with higher alcohol use.  
4. `absences` - number of school absences. A higher number of absences will relate to high alcohol consumption.


## Part 4 - Exploring the Variables

Next, I accessed some R packages that I will need shortly, and I also created a mini-dataset called `alc_dataset`. This mini-dataset will allow me to represent just the data I chose for my analysis.

Accessing packages dplyr and ggplot2:

```{r}
library(dplyr); library(ggplot2)
```

Creating `alc_dataset`:

```{r}
alc_use <- alc$alc_use
high_use <- alc$high_use
age <- alc$age
failures <- alc$failures
famrel <- alc$famrel
absences <- alc$absences
alc_dataset <- data.frame(alc_use, age, failures, famrel, absences)
summary(alc_dataset)
```

Above is the initial numeric summary for all the variables that I chose. This does not tell us anything when it comes to the relationships between the variables, but it is a nice neat summary of the distribution of each variable.


Before we get into the logistic regression in part 5 below, let's look at some numerical (summaries by group using the pipe operator) and graphical (box plot) representations of the data. Here, I will compare the variable `high_use` with variables `age`, `failures`, `famrel`, and `absences`.


### Variable `age`


```{r}
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_age = mean(age))
```


```{r}
# barplot
g1a <- ggplot(data=alc, aes(x=age, y=consumption)) +
geom_bar(stat="identity")

# initialise a plot of high_use and age
g1 <- ggplot(alc, aes(x = high_use, y = age, col = sex))
 
 # define the plot as a boxplot and draw it
g1 + geom_boxplot() + ggtitle("Student ages by alcohol consumption and sex")
```
  
The numerical representation and the plot above quite clearly shows that my first hypothesis, concerning age, was largely rubbish. Moving on!

  
### Variable `failures`

  
```{r}
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_failures = mean(failures))
```


```{r}
# initialise a plot of high_use and failures
g2 <- ggplot(alc, aes(x = high_use, y = failures, col = sex))
 
 # define the plot as a boxplot and draw it
 g2 + geom_boxplot() + ggtitle("Student failures by alcohol consumption and sex")
```

The analysis of `failures` shows that, regardless of the rather useless box plot, my hypothesis was in the right track. In both males and females, the mean failures of those having high alcohol use was considerably more than the mean failures of those not having high alcohol use. In males, the difference between mean failures of high and low alcohol users, at 0.196, is higher than the difference, 0.171, in females. This is starting to point toward the fact that perhaps I should have hypothesized that being male is a factor in high alcohol use.


### Variable `famrel`


```{r}
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_famrel = mean(famrel))
```


```{r}
# initialise a plot of high_use and family relationships
g3 <- ggplot(alc, aes(x = high_use, y = famrel, col = sex))
 
 # define the plot as a boxplot and draw it
 g3 + geom_boxplot() + ggtitle("Student family relationships by alcohol consumption and sex")
```

The analysis of `famrel` also shows that my hypothesis was not incorrect. In both sexes, the mean family relationships score of those having high alcohol use was lower than the mean failures of those not having high alcohol use. Here, a difference between the sexes is also evident, albeit in a manner rather different from the variable `failures`. The mean family relationship scores for the high alcohol users in both sexes was about the same, whereas, rather interrestingly, the family relationships score was better for males in the low alcohol use group.


### Variable `absences`


```{r}
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_absences = mean(absences))
```

```{r}
# initialise a plot of high_use and absences
g4 <- ggplot(alc, aes(x = high_use, y = absences, col = sex))
 
 # define the plot as a boxplot and draw it
 g4 + geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
```

The analysis of `absences` shows that my hypothesis was correct: a higher number of absences does indeed correlate with high alcohol consumption. Again, the differences between the sexes in striking. The mean absences for both male and female high alcohol users are around the same ballpark, but the mean absence figures for low alcohol users differs dramatically between the sexes: 2.982 for the males and 4.224 for the females. This not only indicates that females generally have more school absences, but also that in males the introduction of high alcohol use dramatically increases the mean number of absences.


## Part 5 - Logistic Regression

Next, I will use logistic regression to explore the relationship between my chosen variables and the binary variable `high_use`. This is done by using `glm()` to fit a logictic regression model. I will also print a summary of the model and I will use `coef()` to print out the coefficients of my model.

```{r}
model <- glm(high_use ~ age + failures + famrel + absences, data = alc, family = "binomial")
summary(model)
coef(model)
```

In the fitted model, `absences` shows a significance level `***`, or less than 0.0001 (0.01%) and it is therefore statistically significant. `failures` is also significant with `*` a significance level of around 2%. `famrel` is close to the generally accepted significance level of 5%, whereas, as previously observed, `age` is not statistically significant.

As I observed above, it seems that `sex` has a statistical significance, and I am fitting my model again. This time I will include `sex` and exclude `age`.

```{r}
model <- glm(high_use ~ sex + failures + famrel + absences, data = alc, family = "binomial")
summary(model)
coef(model)
```

As I suspected, the model now looks much better. All the variables have an acceptable level of significance, with the variable `sexM` (male gender) having a significance level on par with `absences`.

Now I will create the object OR (odds ratios) by using `coef()` on the model object to extract the coefficients of the model and then apply the `exp` function on the coefficients. The next step is to use `confint()` on the model  to compute confidence intervals for the coefficients. I will exponentiate the values and assign the results to the object `CI`. Finally, I will combine `cbind` and print out the odds ratios and their confidence intervals.

```{r}
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)

```

To be able to meaningfully interpret the coefficients of the model as odds ratios we need to know when odds ratios are used. They are used to compare the relative odds of the occurrence of the outcome of interest (in this case high alcohol use), given exposure to the variable of interest.

The odds ratios are interpreted as follows:

OR=1 Exposure does not affect odds of outcome
OR>1 Exposure associated with higher odds of outcome
OR<1 Exposure associated with lower odds of outcome

We therefore can see that exposure to `famrel` is associated with lower odds of outcome, whereas exposure to `absences`, `failures`, and especially `sexM` is associated with higher odds of the outcome `high_use`. `sexM` also has the widest confidence interval of all the variables.

If the confidence interval crosses 1 (contains the value of no effect), this implies that the observed effect is statistically not significant. This, as can be seen from the table above, is not the case for any of the variables.

As stated before, my hypothesis regarding the variable `age` was wrong, and as soon as it became apparent that a person being male was significant, I added the variable `sexM` to my model. The four variables in the fitted model all have a statistically significant relationship with high alcohol use in the following descending order: `sexM`, `failures`, `absences`, and `famrel`.

## Part 6 - Exploring the Predictive Power of My Model

According to my logistic regression model, `sexM`, `failures`, `famrel` and `absences` all had a statistical relationship with high/low alcohol use. Therefore, I do not need to make any adjustments to my model at this stage - as I did so in the step above.

Below I explore the predictive power of my model by providing a 2x2 cross tabulation of predictions versus the actual values.

```{r}
probabilities <- predict(model, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)
select(alc, failures, absences, famrel, sex, high_use, probability, prediction) %>% tail(10)
table(high_use = alc$high_use, prediction = alc$prediction)
```

Let's also draw a plot to examine the predictions and create a cross table of `high_use` versus `prediction`.

```{r}
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))
g + geom_point()
table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table %>% addmargins
```
The table above illustrates how accurate my predictions are. By adding the figures for high_use being FALSE while the model is predicting TRUE and high_use being TRUE while the model predicts FALSE we can calculate the percentage of all cases where my model would be wrong: 19.9%. Therefore, the model would be right roughly 80% of the time, which is much better a result than we would achieve by simple guessing with equal weights. The greatest delta between the number of observed and expected respondents is FALSE and FALSE at 63.9%, and my model comfortably outperforms this too, indicating that my model has some validity.

The next task at hand is accuracy and loss functions. I will compute the total proportion of inaccurately classified individuals, in other words the training error. The loss function `loss_func` is defined and called to compute the average number of wrong predictions in the (training) data.

```{r}
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
loss_func(class = alc$high_use, prob = alc$probability)
```

The training error for this model is approximately 0.254, while the test error is calculated in the following section. 


## Part 7 - Bonus: 10-fold Cross-validation

As a bonus I will perform a 10-old cross-validation on my model. This tells us the testing errors, as opposed to the training errors above.


```{r}
# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = model, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```

My model has a very slightly better test set performance than the model that was introduced in DataCamp: 0.2617801 versus 0.2643979. The testing error of my model is higher than my training error. A lower training error is expected when a method easily overfits to the training data. 

## Part 8 - Super-Bonus: Comparing the Performance of Different Logistic Regression Models

As a super-bonus I will perform cross-validation to compare the performance of different logistic regression models. I will start with the following 20 predictors: `sex`, `address`, `famsize`, `Pstatus`, `Medu`, `Fedu`, `Mjob`, `Fjob`,  `guardian`, `studytime`, `failures`, `schoolsup`, `famsup`, `activities`, `higher`, `romantic`, `famrel`, `goout`, `health`, and `absences`.

```{r}
alc_use <- alc$alc_use
high_use <- alc$high_use
sex <- alc$sex
address <- alc$address
famsize <- alc$famsize
Pstatus <- alc$Pstatus
Medu <- alc$Medu
Fedu <- alc$Fedu
Mjob <- alc$Mjob
Fjob <- alc$Fjob
guardian <- alc$guardian
studytime <- alc$studytime
failures <- alc$failures
schoolsup <- alc$schoolsup
famsup <- alc$famsup
activities <- alc$activities
higher <- alc$higher
romantic <- alc$romantic
famrel <- alc$famrel
goout <- alc$goout
health <- alc$health
absences <- alc$absences

model_20 <- glm(high_use ~ sex + address + famsize + Pstatus + Medu + Fedu + Mjob + Fjob + guardian + studytime + failures + schoolsup + famsup + activities + higher + romantic + famrel + goout + health + absences, data = alc, family = "binomial")

probabilities <- predict(model_20, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_20 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_20

cv_20 <- cv.glm(data = alc, cost = loss_func, glmfit = model_20, K = 10)
ts_er_20 <- cv_20$delta[1]
ts_er_20
```

Let's see what happens when we reduce the number of predictors to 15.

I will leave out `higher`, `famsup`, `schoolsup`, `Medu` and `Fedu` from the next model, leaving me with 15 predictors.

```{r}
model_15 <- glm(high_use ~ sex + address + famsize + Pstatus + Mjob + Fjob + guardian + studytime + failures + activities + romantic + famrel + goout + health + absences, data = alc, family = "binomial")

probabilities <- predict(model_15, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_15 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_15

cv_15 <- cv.glm(data = alc, cost = loss_func, glmfit = model_15, K = 10)
ts_er_15 <- cv_15$delta[1]
ts_er_15
```

Now I will reduce the number of predictors to 10.

I will leave out `famsize`, `Pstatus`, `Mjob`, `Fjob` and `romantic` from the next model, leaving me with 10 predictors.

```{r}
model_10 <- glm(high_use ~ sex + address + guardian + studytime + failures + activities + famrel + goout + health + absences, data = alc, family = "binomial")

probabilities <- predict(model_10, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_10 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_10

cv_10 <- cv.glm(data = alc, cost = loss_func, glmfit = model_10, K = 10)
ts_er_10 <- cv_10$delta[1]
ts_er_10
```

Now I will reduce the number of predictors to 6.

I will take out 4 predictors (`activities`, `guardian`, `failures` and `health`). 

```{r}
model_6 <- glm(high_use ~ sex + address + studytime + famrel + goout + absences, data = alc, family = "binomial")

probabilities <- predict(model_6, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_6 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_6

cv_6 <- cv.glm(data = alc, cost = loss_func, glmfit = model_6, K = 10)
ts_er_6 <- cv_6$delta[1]
ts_er_6
```

I will now take out the predictors one by one, until I reach 1.

```{r}
model_5 <- glm(high_use ~ sex + studytime + famrel + goout + absences, data = alc, family = "binomial")

probabilities <- predict(model_5, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_5 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_5

cv_5 <- cv.glm(data = alc, cost = loss_func, glmfit = model_5, K = 10)
ts_er_5 <- cv_5$delta[1]
ts_er_5
```
 
```{r}
model_4 <- glm(high_use ~ sex + famrel + goout + absences, data = alc, family = "binomial")

probabilities <- predict(model_4, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_4 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_4

cv_4 <- cv.glm(data = alc, cost = loss_func, glmfit = model_4, K = 10)
ts_er_4 <- cv_4$delta[1]
ts_er_4
```

```{r}
model_3 <- glm(high_use ~ sex + goout + absences, data = alc, family = "binomial")

probabilities <- predict(model_3, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_3 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_3

cv_3 <- cv.glm(data = alc, cost = loss_func, glmfit = model_3, K = 10)
ts_er_3 <- cv_3$delta[1]
ts_er_3
```

```{r}
model_2 <- glm(high_use ~ sex + absences, data = alc, family = "binomial")

probabilities <- predict(model_2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_2 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_2

cv_2 <- cv.glm(data = alc, cost = loss_func, glmfit = model_2, K = 10)
ts_er_2 <- cv_2$delta[1]
ts_er_2
```
```{r}
model_1 <- glm(high_use ~ sex, data = alc, family = "binomial")

probabilities <- predict(model_1, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
t_er_1 <- loss_func(class = alc$high_use, prob = alc$probability)
t_er_1

cv_1 <- cv.glm(data = alc, cost = loss_func, glmfit = model_1, K = 10)
ts_er_1 <- cv_1$delta[1]
ts_er_1
```

Finally, I drew a simple graph that displays the trends of both training and testing errors by the number of predictors in the model.

```{r}
t_er<-c(t_er_1,t_er_2,t_er_3,t_er_4,t_er_5,t_er_6,t_er_10,t_er_15,t_er_20)

ts_er<-c(ts_er_1,ts_er_2,ts_er_3,ts_er_4,ts_er_5,ts_er_6,ts_er_10,ts_er_15,ts_er_20)

plot(t_er, type="o", col="blue", xlab="Number of predictors / model complexity", ylab="Prediction error")
lines(ts_er, type="o", col="red")
axis(1, at=1:9, lab=c("1","2","3","4","5","6","10","15","20"))
title(main="Training and test errors by number of predictors")
legend("topright", c("training error","test error"),lty=c(1,1), lwd=c(2.5,2.5),col=c("blue","red")) 

```

Overfitting will occur as soon as the test error starts to increase while the training error is still decreasing. In my model the sweet spot is at 10 predictors, anything more than that and the model will overfit, possibly as it is too complex (possibly, because there are many more reasons for overfitting).

To the left of the graph are situations where the model is low in complexity and has high bias and low variance.
To the right of the graph are situations where the model is high in complexity and has low bias and high variance.

That's all for this week - I'm already looking forward to next week's tasks!
